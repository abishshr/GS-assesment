{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed056883-2717-4854-802c-861e458ca183",
   "metadata": {},
   "source": [
    "### Input Data: Metric, Value and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b279b502-e399-4177-b0e3-502ee3702b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Metric', 'Value', 'Timestamp']\n",
    "rows = [\n",
    "    ['temperature', 88, '2022-06-04T12:01:00.000Z'],\n",
    "    ['temperature', 89, '2022-06-04T12:01:30.000Z'],\n",
    "    ['precipitation', 0.5, '2022-06-04T14:23:32.000Z'],\n",
    "    ['temperature', 84, '2022-06-04T13:02:00.000Z'],\n",
    "    ['temperature', 86, '2022-06-04T13:03:00.000Z'],\n",
    "    ['precipitation', 0.2, '2022-06-04T14:24:32.000Z'],\n",
    "    ['temperature', 91, '2022-06-04T15:05:00.000Z'],\n",
    "    ['precipitation', 0.8, '2022-06-04T15:30:32.000Z'],\n",
    "    ['temperature', 92, '2022-06-04T16:06:00.000Z'],\n",
    "    ['temperature', 94, '2022-06-04T17:07:00.000Z'],\n",
    "    ['precipitation', 0.7, '2022-06-04T17:45:32.000Z'],\n",
    "    ['temperature', 90, '2022-06-04T18:08:00.000Z'],\n",
    "    ['precipitation', 0.6, '2022-06-04T18:20:32.000Z']\n",
    "]\n",
    "\n",
    "with open('input_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be42db8-02c2-4369-aceb-c95c326ce447",
   "metadata": {},
   "source": [
    "### Setting Up Spark Environment for Batch Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da2d981c-9be3-47a1-904b-54fb6a0a5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# create a spark context\n",
    "sc = SparkContext(\"local\", \"Batch Aggregation App\")\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960455e-627a-4971-91f1-92697fc8d26f",
   "metadata": {},
   "source": [
    "### Loading the Source Data into Datframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "40ad02d0-68ea-40e3-8e95-698ba908f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------------+\n",
      "|       Metric|Value|          Timestamp|\n",
      "+-------------+-----+-------------------+\n",
      "|precipitation|  0.5|2022-06-04 10:23:32|\n",
      "|precipitation|  0.2|2022-06-04 10:24:32|\n",
      "|precipitation|  0.8|2022-06-04 11:30:32|\n",
      "|precipitation|  0.7|2022-06-04 13:45:32|\n",
      "|precipitation|  0.6|2022-06-04 14:20:32|\n",
      "|  temperature|   88|2022-06-04 08:01:00|\n",
      "|  temperature|   89|2022-06-04 08:01:30|\n",
      "|  temperature|   84|2022-06-04 09:02:00|\n",
      "|  temperature|   86|2022-06-04 09:03:00|\n",
      "|  temperature|   91|2022-06-04 11:05:00|\n",
      "|  temperature|   92|2022-06-04 12:06:00|\n",
      "|  temperature|   94|2022-06-04 13:07:00|\n",
      "|  temperature|   90|2022-06-04 14:08:00|\n",
      "+-------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max, min, to_utc_timestamp, floor\n",
    "from pyspark.sql.functions import to_utc_timestamp\n",
    "\n",
    "# read input data from csv file\n",
    "df = spark.read.csv(\"input_data.csv\", header=True)\n",
    "\n",
    "# convert timestamp column to utc timestamp format\n",
    "df = df.withColumn(\"Timestamp\", to_utc_timestamp(df[\"Timestamp\"], \"UTC\"))\n",
    "\n",
    "# performance optimization as it can reduce the time required to perform subsequent operations \n",
    "df = df.orderBy(\"Metric\", \"Timestamp\")\n",
    "df.persist()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f15cf-1e00-4efe-b5d5-5979381c3234",
   "metadata": {},
   "source": [
    "### Transformations/Business Loigic : Aggregating Values for Each Day and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "384f6e05-73f6-454b-a07c-b933369529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a window based on the Timestamp column, with a duration of 1 day and a slide duration of 1 day\n",
    "window = F.window(\n",
    "    F.col(\"Timestamp\"),\n",
    "    windowDuration=\"1 day\",\n",
    "    slideDuration=\"1 day\",\n",
    ").alias(\"window\")\n",
    "\n",
    "\n",
    "\n",
    "# aggergated values \n",
    "df_aggregated = df.withColumn(\"window\", window)\\\n",
    "                    .groupBy(\"window\", \"Metric\")\\\n",
    "                    .agg(\n",
    "                        F.avg(\"Value\").alias(\"avg_value\"),\n",
    "                        F.min(\"Value\").alias(\"min_value\"),\n",
    "                        F.max(\"Value\").alias(\"max_value\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b72046-bb88-4af9-9cfe-95028418b4d5",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4827f6e7-e8f3-4ded-953c-e602c80659cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------------+---------+---------+---------+\n",
      "|window                                    |Metric       |avg_value|min_value|max_value|\n",
      "+------------------------------------------+-------------+---------+---------+---------+\n",
      "|{2022-06-03 20:00:00, 2022-06-04 20:00:00}|precipitation|0.56     |0.2      |0.8      |\n",
      "|{2022-06-03 20:00:00, 2022-06-04 20:00:00}|temperature  |89.25    |84       |94       |\n",
      "+------------------------------------------+-------------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aggregated.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "70388d95-a26c-4d9d-80e4-5528b08105af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c80223df-5bcd-4e9f-afa6-c1ce01a36936",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "CSV data source does not support struct<start:timestamp,end:timestamp> data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/18/cbt47sd946n9mmffxypsx6k40000gn/T/ipykernel_36962/2769498706.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_aggregated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aggregated_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def orc(\n",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support struct<start:timestamp,end:timestamp> data type."
     ]
    }
   ],
   "source": [
    "df_aggregated.write.csv(\"aggregated_data.csv\", mode='overwrite', header=True, timestampFormat=\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1421b59-5e18-4e38-937f-670d61363a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "df_aggregated = df_aggregated.withColumn(\"window\", concat(\n",
    "    col(\"window.start\").cast(\"string\"),\n",
    "    F.lit(\" - \"),\n",
    "    col(\"window.end\").cast(\"string\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f23a682-4747-4d80-89d5-75421f49e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated.write.csv(\"./aggregated_data.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49616030-6a4e-4097-ad40-dda8f60f8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[window: string, Metric: string, avg_value: double, min_value: string, max_value: string]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc6966-b42f-49a6-b0bb-24bf40c0020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
