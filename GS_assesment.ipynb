{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed056883-2717-4854-802c-861e458ca183",
   "metadata": {},
   "source": [
    "### Input Data: Metric, Value and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b279b502-e399-4177-b0e3-502ee3702b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Metric', 'Value', 'Timestamp']\n",
    "rows = [\n",
    "    ['temperature', 88, '2022-06-04T12:01:00.000Z'],\n",
    "    ['temperature', 89, '2022-06-04T12:01:30.000Z'],\n",
    "    ['precipitation', 0.5, '2022-06-04T14:23:32.000Z'],\n",
    "    ['temperature', 84, '2022-06-04T13:02:00.000Z'],\n",
    "    ['temperature', 86, '2022-06-04T13:03:00.000Z'],\n",
    "    ['precipitation', 0.2, '2022-06-04T14:24:32.000Z'],\n",
    "    ['temperature', 91, '2022-06-04T15:05:00.000Z'],\n",
    "    ['precipitation', 0.8, '2022-06-04T15:30:32.000Z'],\n",
    "    ['temperature', 92, '2022-06-04T16:06:00.000Z'],\n",
    "    ['temperature', 94, '2022-06-04T17:07:00.000Z'],\n",
    "    ['precipitation', 0.7, '2022-06-04T17:45:32.000Z'],\n",
    "    ['temperature', 90, '2022-06-04T18:08:00.000Z'],\n",
    "    ['precipitation', 0.6, '2022-06-04T18:20:32.000Z']\n",
    "]\n",
    "\n",
    "with open('input_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be42db8-02c2-4369-aceb-c95c326ce447",
   "metadata": {},
   "source": [
    "### Setting Up Spark Environment for Batch Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "da2d981c-9be3-47a1-904b-54fb6a0a5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# create a spark context\n",
    "sc = SparkContext(\"local\", \"Batch Aggregation App\")\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960455e-627a-4971-91f1-92697fc8d26f",
   "metadata": {},
   "source": [
    "### Loading the Source Data into Datframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "40ad02d0-68ea-40e3-8e95-698ba908f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------------+\n",
      "|       Metric|Value|          Timestamp|\n",
      "+-------------+-----+-------------------+\n",
      "|precipitation|  0.5|2022-06-04 10:23:32|\n",
      "|precipitation|  0.2|2022-06-04 10:24:32|\n",
      "|precipitation|  0.8|2022-06-04 11:30:32|\n",
      "|precipitation|  0.7|2022-06-04 13:45:32|\n",
      "|precipitation|  0.6|2022-06-04 14:20:32|\n",
      "|  temperature|   88|2022-06-04 08:01:00|\n",
      "|  temperature|   89|2022-06-04 08:01:30|\n",
      "|  temperature|   84|2022-06-04 09:02:00|\n",
      "|  temperature|   86|2022-06-04 09:03:00|\n",
      "|  temperature|   91|2022-06-04 11:05:00|\n",
      "|  temperature|   92|2022-06-04 12:06:00|\n",
      "|  temperature|   94|2022-06-04 13:07:00|\n",
      "|  temperature|   90|2022-06-04 14:08:00|\n",
      "+-------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max, min, to_utc_timestamp, floor\n",
    "from pyspark.sql.functions import to_utc_timestamp\n",
    "\n",
    "# read input data from csv file\n",
    "df = spark.read.csv(\"input_data.csv\", header=True)\n",
    "\n",
    "# convert timestamp column to utc timestamp format\n",
    "df = df.withColumn(\"Timestamp\", to_utc_timestamp(df[\"Timestamp\"], \"UTC\"))\n",
    "\n",
    "# performance optimization as it can reduce the time required to perform subsequent operations \n",
    "df = df.orderBy(\"Metric\", \"Timestamp\")\n",
    "df.persist()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f15cf-1e00-4efe-b5d5-5979381c3234",
   "metadata": {},
   "source": [
    "### Transformations/Business Loigic : Aggregating Values for Each Day and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "384f6e05-73f6-454b-a07c-b933369529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a window based on the Timestamp column, with a duration of 1 day and a slide duration of 1 day\n",
    "window = F.window(\n",
    "    F.col(\"Timestamp\"),\n",
    "    windowDuration=\"1 day\",\n",
    "    slideDuration=\"1 day\",\n",
    ").alias(\"window\")\n",
    "\n",
    "\n",
    "\n",
    "# aggergated values \n",
    "df_aggregated = df.withColumn(\"window\", window)\\\n",
    "                    .groupBy(\"window\", \"Metric\")\\\n",
    "                    .agg(\n",
    "                        F.avg(\"Value\").alias(\"avg_value\"),\n",
    "                        F.min(\"Value\").alias(\"min_value\"),\n",
    "                        F.max(\"Value\").alias(\"max_value\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b72046-bb88-4af9-9cfe-95028418b4d5",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4827f6e7-e8f3-4ded-953c-e602c80659cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------------+---------+---------+---------+\n",
      "|window                                    |Metric       |avg_value|min_value|max_value|\n",
      "+------------------------------------------+-------------+---------+---------+---------+\n",
      "|{2022-06-03 20:00:00, 2022-06-04 20:00:00}|precipitation|0.56     |0.2      |0.8      |\n",
      "|{2022-06-03 20:00:00, 2022-06-04 20:00:00}|temperature  |89.25    |84       |94       |\n",
      "+------------------------------------------+-------------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aggregated.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1dc4c-3d61-4ddb-a6b0-bc690ad60ba8",
   "metadata": {},
   "source": [
    "### Save Output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e1421b59-5e18-4e38-937f-670d61363a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "# Convert window to string \n",
    "df_aggregated = df_aggregated.withColumn(\"window\", concat(\n",
    "    col(\"window.start\").cast(\"string\"),\n",
    "    F.lit(\" - \"),\n",
    "    col(\"window.end\").cast(\"string\")\n",
    "))\n",
    "# save the file \n",
    "df_aggregated.write.csv(\"./GS-assesment/aggregated_data.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a4bc6966-b42f-49a6-b0bb-24bf40c0020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b553e16-f589-4aa7-b591-6a241b6081dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
